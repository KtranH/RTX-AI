{
  "1": {
    "inputs": {
      "ckpt_name": "xxmixunreal_v10.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "2": {
    "inputs": {
      "text": "1girl, curly hair, purple hair",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "3": {
    "inputs": {
      "text": "embedding:ng_deepnegative_v1_75t, embedding:badhandv4, embedding:negative_hand-neg, embedding:bad-picture-chill-75v, embedding:verybadimagenegative_v1.3, ",
      "clip": [
        "1",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "4": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "5": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "6": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 49266212675938,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "7",
        0
      ],
      "positive": [
        "2",
        0
      ],
      "negative": [
        "3",
        0
      ],
      "latent_image": [
        "5",
        0
      ],
      "optional_vae": [
        "4",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "7": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "1",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "8": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 700228397197771,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "6",
        5
      ],
      "model": [
        "6",
        0
      ],
      "positive": [
        "6",
        1
      ],
      "negative": [
        "6",
        2
      ],
      "vae": [
        "6",
        4
      ],
      "upscale_model": [
        "9",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "9": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "10": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 110230281718223,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "8",
        0
      ],
      "model": [
        "6",
        0
      ],
      "clip": [
        "1",
        1
      ],
      "vae": [
        "6",
        4
      ],
      "positive": [
        "6",
        1
      ],
      "negative": [
        "6",
        2
      ],
      "bbox_detector": [
        "12",
        0
      ],
      "sam_model_opt": [
        "11",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "11": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "12": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "13": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "10",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "15": {
    "inputs": {
      "ckpt_name": "xxmixunreal_v10.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "16": {
    "inputs": {
      "text": "1girl, curly hair, purple hair",
      "clip": [
        "15",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "17": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:verybadimagenegative_v1.3, embedding:bad-picture-chill-75v, ",
      "clip": [
        "15",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "18": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "19": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "20": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 1023384462677975,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "32",
        0
      ],
      "positive": [
        "16",
        0
      ],
      "negative": [
        "17",
        0
      ],
      "latent_image": [
        "19",
        0
      ],
      "optional_vae": [
        "18",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "21": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "15",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "22": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 528223083057021,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "20",
        5
      ],
      "model": [
        "20",
        0
      ],
      "positive": [
        "20",
        1
      ],
      "negative": [
        "20",
        2
      ],
      "vae": [
        "20",
        4
      ],
      "upscale_model": [
        "23",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "23": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "24": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 682904906589102,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "22",
        0
      ],
      "model": [
        "20",
        0
      ],
      "clip": [
        "15",
        1
      ],
      "vae": [
        "20",
        4
      ],
      "positive": [
        "20",
        1
      ],
      "negative": [
        "20",
        2
      ],
      "bbox_detector": [
        "26",
        0
      ],
      "sam_model_opt": [
        "25",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "25": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "26": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "27": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "24",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "28": {
    "inputs": {
      "image": "022e08eb-c7d3-41ca-b53c-1905d7210fbe.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "29": {
    "inputs": {
      "preset": "LIGHT - SD1.5 only (low strength)",
      "model": [
        "21",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "30": {
    "inputs": {
      "weight": 0.8,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "29",
        0
      ],
      "ipadapter": [
        "29",
        1
      ],
      "image": [
        "28",
        0
      ],
      "clip_vision": [
        "33",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "31": {
    "inputs": {
      "preset": "STANDARD (medium strength)",
      "model": [
        "30",
        0
      ],
      "ipadapter": [
        "29",
        1
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "32": {
    "inputs": {
      "weight": 0.8,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "31",
        0
      ],
      "ipadapter": [
        "31",
        1
      ],
      "image": [
        "28",
        0
      ],
      "clip_vision": [
        "33",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "33": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "35": {
    "inputs": {
      "ckpt_name": "xxmixunreal_v10.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "36": {
    "inputs": {
      "text": "1girl, curly hair, purple hair",
      "clip": [
        "35",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "37": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:verybadimagenegative_v1.3, embedding:bad-picture-chill-75v, ",
      "clip": [
        "35",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "38": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "39": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "40": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 1023384462677975,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "59",
        0
      ],
      "positive": [
        "36",
        0
      ],
      "negative": [
        "37",
        0
      ],
      "latent_image": [
        "39",
        0
      ],
      "optional_vae": [
        "38",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "41": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "35",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "42": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 1109613446243839,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "40",
        5
      ],
      "model": [
        "40",
        0
      ],
      "positive": [
        "40",
        1
      ],
      "negative": [
        "40",
        2
      ],
      "vae": [
        "40",
        4
      ],
      "upscale_model": [
        "43",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "43": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "44": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 129585114084817,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "42",
        0
      ],
      "model": [
        "40",
        0
      ],
      "clip": [
        "35",
        1
      ],
      "vae": [
        "40",
        4
      ],
      "positive": [
        "40",
        1
      ],
      "negative": [
        "40",
        2
      ],
      "bbox_detector": [
        "46",
        0
      ],
      "sam_model_opt": [
        "45",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "45": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "46": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "47": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "44",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "48": {
    "inputs": {
      "image": "ComfyUI_00867_.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Main IMG"
    }
  },
  "49": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "50": {
    "inputs": {
      "image": "022e08eb-c7d3-41ca-b53c-1905d7210fbe.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Other IMG"
    }
  },
  "51": {
    "inputs": {
      "ipadapter_file": "ip-adapter-plus_sd15.safetensors"
    },
    "class_type": "IPAdapterModelLoader",
    "_meta": {
      "title": "IPAdapter Model Loader"
    }
  },
  "52": {
    "inputs": {
      "interpolation": "LANCZOS",
      "crop_position": "top",
      "sharpening": 0.15
    },
    "class_type": "PrepImageForClipVision",
    "_meta": {
      "title": "Prep Image For ClipVision"
    }
  },
  "53": {
    "inputs": {
      "interpolation": "LANCZOS",
      "crop_position": "top",
      "sharpening": 0.15
    },
    "class_type": "PrepImageForClipVision",
    "_meta": {
      "title": "Prep Image For ClipVision"
    }
  },
  "54": {
    "inputs": {
      "weight": 1,
      "ipadapter": [
        "51",
        0
      ],
      "image": [
        "52",
        0
      ],
      "clip_vision": [
        "49",
        0
      ]
    },
    "class_type": "IPAdapterEncoder",
    "_meta": {
      "title": "IPAdapter Encoder"
    }
  },
  "55": {
    "inputs": {
      "weight": 1,
      "ipadapter": [
        "51",
        0
      ],
      "image": [
        "53",
        0
      ],
      "clip_vision": [
        "49",
        0
      ]
    },
    "class_type": "IPAdapterEncoder",
    "_meta": {
      "title": "IPAdapter Encoder"
    }
  },
  "56": {
    "inputs": {
      "method": "average",
      "embed1": [
        "54",
        0
      ],
      "embed2": [
        "55",
        0
      ]
    },
    "class_type": "IPAdapterCombineEmbeds",
    "_meta": {
      "title": "IPAdapter Combine Embeds"
    }
  },
  "57": {
    "inputs": {
      "type": "shuffle",
      "strength": 0.4,
      "blur": 0,
      "image_optional": [
        "50",
        0
      ]
    },
    "class_type": "IPAdapterNoise",
    "_meta": {
      "title": "IPAdapter Noise"
    }
  },
  "58": {
    "inputs": {
      "weight": 1,
      "ipadapter": [
        "51",
        0
      ],
      "image": [
        "57",
        0
      ],
      "clip_vision": [
        "49",
        0
      ]
    },
    "class_type": "IPAdapterEncoder",
    "_meta": {
      "title": "IPAdapter Encoder"
    }
  },
  "59": {
    "inputs": {
      "weight": 0.8,
      "weight_type": "linear",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "41",
        0
      ],
      "ipadapter": [
        "51",
        0
      ],
      "pos_embed": [
        "56",
        0
      ],
      "neg_embed": [
        "58",
        0
      ],
      "clip_vision": [
        "49",
        0
      ]
    },
    "class_type": "IPAdapterEmbeds",
    "_meta": {
      "title": "IPAdapter Embeds"
    }
  },
  "61": {
    "inputs": {
      "ckpt_name": "xxmixunreal_v10.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "62": {
    "inputs": {
      "text": "1girl, curly hair, purple hair",
      "clip": [
        "61",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "63": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:verybadimagenegative_v1.3, embedding:bad-picture-chill-75v, ",
      "clip": [
        "61",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "64": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "65": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "66": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 1023384462677975,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "67",
        0
      ],
      "positive": [
        "78",
        0
      ],
      "negative": [
        "63",
        0
      ],
      "latent_image": [
        "65",
        0
      ],
      "optional_vae": [
        "64",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "67": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "61",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "68": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 494107108087888,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "66",
        5
      ],
      "model": [
        "66",
        0
      ],
      "positive": [
        "66",
        1
      ],
      "negative": [
        "66",
        2
      ],
      "vae": [
        "66",
        4
      ],
      "upscale_model": [
        "69",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "69": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "70": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 932612475513472,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "68",
        0
      ],
      "model": [
        "66",
        0
      ],
      "clip": [
        "61",
        1
      ],
      "vae": [
        "66",
        4
      ],
      "positive": [
        "66",
        1
      ],
      "negative": [
        "66",
        2
      ],
      "bbox_detector": [
        "72",
        0
      ],
      "sam_model_opt": [
        "71",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "71": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "72": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "73": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "70",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "74": {
    "inputs": {
      "image": "34188f795ce317d3748ca3c684fc2f65.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "75": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "76": {
    "inputs": {
      "preprocessor": "Scribble_XDoG_Preprocessor",
      "resolution": 512,
      "image": [
        "74",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "AIO Aux Preprocessor"
    }
  },
  "77": {
    "inputs": {
      "images": [
        "76",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "78": {
    "inputs": {
      "strength": 1,
      "conditioning": [
        "62",
        0
      ],
      "control_net": [
        "79",
        0
      ],
      "image": [
        "76",
        0
      ]
    },
    "class_type": "ControlNetApply",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "79": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_scribble_fp16.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "81": {
    "inputs": {
      "ckpt_name": "xxmixunreal_v10.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "82": {
    "inputs": {
      "text": [
        "102",
        0
      ],
      "clip": [
        "81",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "83": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:verybadimagenegative_v1.3, embedding:bad-picture-chill-75v, ",
      "clip": [
        "81",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "84": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "85": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "86": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 1023384462677975,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "101",
        0
      ],
      "positive": [
        "98",
        0
      ],
      "negative": [
        "83",
        0
      ],
      "latent_image": [
        "85",
        0
      ],
      "optional_vae": [
        "84",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "87": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "81",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "88": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 1043896385350996,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "86",
        5
      ],
      "model": [
        "86",
        0
      ],
      "positive": [
        "86",
        1
      ],
      "negative": [
        "86",
        2
      ],
      "vae": [
        "86",
        4
      ],
      "upscale_model": [
        "89",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "89": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "90": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 121629885977698,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "88",
        0
      ],
      "model": [
        "86",
        0
      ],
      "clip": [
        "81",
        1
      ],
      "vae": [
        "86",
        4
      ],
      "positive": [
        "86",
        1
      ],
      "negative": [
        "86",
        2
      ],
      "bbox_detector": [
        "92",
        0
      ],
      "sam_model_opt": [
        "91",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "91": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "92": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "93": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "90",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "94": {
    "inputs": {
      "image": "432631010_975986497217737_5885795469822289220_n.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "95": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "96": {
    "inputs": {
      "preprocessor": "DepthAnythingV2Preprocessor",
      "resolution": 512,
      "image": [
        "94",
        0
      ]
    },
    "class_type": "AIO_Preprocessor",
    "_meta": {
      "title": "AIO Aux Preprocessor"
    }
  },
  "97": {
    "inputs": {
      "images": [
        "96",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "98": {
    "inputs": {
      "strength": 1,
      "conditioning": [
        "82",
        0
      ],
      "control_net": [
        "99",
        0
      ],
      "image": [
        "96",
        0
      ]
    },
    "class_type": "ControlNetApply",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "99": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_scribble_fp16.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "100": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "87",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "101": {
    "inputs": {
      "weight": 1,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "100",
        0
      ],
      "ipadapter": [
        "100",
        1
      ],
      "image": [
        "94",
        0
      ],
      "clip_vision": [
        "95",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "102": {
    "inputs": {
      "model": "wd-v1-4-moat-tagger-v2",
      "threshold": 0.35,
      "character_threshold": 0.85,
      "replace_underscore": false,
      "trailing_comma": false,
      "exclude_tags": "",
      "image": [
        "94",
        0
      ]
    },
    "class_type": "WD14Tagger|pysssss",
    "_meta": {
      "title": "WD14 Tagger 🐍"
    }
  },
  "104": {
    "inputs": {
      "ckpt_name": "tmndMix_tmndMixSPRAINBOW.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "105": {
    "inputs": {
      "text": "a dog, smile dog, cute dog, no human, out door, cloud",
      "clip": [
        "117",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "106": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:verybadimagenegative_v1.3, embedding:bad-picture-chill-75v, human, 1girl, 1boy, girl, boy, girls, boys",
      "clip": [
        "104",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "107": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "108": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "109": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 633822643786616,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "110",
        0
      ],
      "positive": [
        "105",
        0
      ],
      "negative": [
        "106",
        0
      ],
      "latent_image": [
        "108",
        0
      ],
      "optional_vae": [
        "107",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "110": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 784,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "117",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "111": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 83890134193095,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "109",
        5
      ],
      "model": [
        "109",
        0
      ],
      "positive": [
        "109",
        1
      ],
      "negative": [
        "109",
        2
      ],
      "vae": [
        "109",
        4
      ],
      "upscale_model": [
        "112",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "112": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "113": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 1111233465754275,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "111",
        0
      ],
      "model": [
        "109",
        0
      ],
      "clip": [
        "104",
        1
      ],
      "vae": [
        "109",
        4
      ],
      "positive": [
        "109",
        1
      ],
      "negative": [
        "109",
        2
      ],
      "bbox_detector": [
        "115",
        0
      ],
      "sam_model_opt": [
        "114",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "114": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "115": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "116": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "113",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "117": {
    "inputs": {
      "lora_name": "MW_Zodiac_v1.0.safetensors",
      "strength_model": 0.8,
      "strength_clip": 0.8,
      "model": [
        "104",
        0
      ],
      "clip": [
        "104",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "119": {
    "inputs": {
      "prompt": "\"1girl, big breasts, bikini\" Are these words sensitive or pornographic? Answer yes or no\n",
      "debug": "enable",
      "url": "http://127.0.0.1:11434",
      "model": "llava:latest",
      "keep_alive": 5
    },
    "class_type": "OllamaGenerate",
    "_meta": {
      "title": "Ollama Generate"
    }
  },
  "120": {
    "inputs": {
      "root_dir": "input",
      "file": "file.txt",
      "append": "overwrite",
      "insert": true,
      "text": [
        "119",
        0
      ]
    },
    "class_type": "SaveText|pysssss",
    "_meta": {
      "title": "Save Text 🐍"
    }
  },
  "121": {
    "inputs": {
      "text": [
        "120",
        0
      ],
      "PreviewTextNode_0": "Yes."
    },
    "class_type": "PreviewTextNode",
    "_meta": {
      "title": "Preview Text Node"
    }
  },
  "122": {
    "inputs": {
      "query": "describe the image",
      "debug": "enable",
      "url": "http://127.0.0.1:11434",
      "model": "llava:latest",
      "keep_alive": 5,
      "images": [
        "125",
        0
      ]
    },
    "class_type": "OllamaVision",
    "_meta": {
      "title": "Ollama Vision"
    }
  },
  "123": {
    "inputs": {
      "root_dir": "input",
      "file": "file.txt",
      "append": "overwrite",
      "insert": true,
      "text": [
        "122",
        0
      ]
    },
    "class_type": "SaveText|pysssss",
    "_meta": {
      "title": "Save Text 🐍"
    }
  },
  "124": {
    "inputs": {
      "text": [
        "123",
        0
      ],
      "PreviewTextNode_1": ""
    },
    "class_type": "PreviewTextNode",
    "_meta": {
      "title": "Preview Text Node"
    }
  },
  "125": {
    "inputs": {
      "image": "022e08eb-c7d3-41ca-b53c-1905d7210fbe.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "126": {
    "inputs": {
      "text": [
        "121",
        0
      ],
      "PreviewTextNode_2": ""
    },
    "class_type": "PreviewTextNode",
    "_meta": {
      "title": "Preview Text Node"
    }
  },
  "129": {
    "inputs": {
      "image": "ComfyUI_temp_yomrb_00023_.png",
      "upload": "image",
      "parameter_index": 0,
      "positive": "",
      "negative": "worst quality,low quality,normal quality,lowres,bad anatomy,bad hands,normal quality,monochrome,grayscale,watermark,\n(embedding:easynegative:1)\n(embedding:ng_deepnegative_v1_75t:1)",
      "setting": "Steps: (50, 50), Sampler: (dpmpp_2m, dpmpp_2m), CFG scale: (5.0, 5.0), Seed: (457211852013219, 219580459669310), Size: 1318x1840, Model: (xxmix9realistic_v40.safetensors, xxmix9realistic_v40.safetensors), Scheduler: (karras, karras), Denoise: (1.0, 0.5)"
    },
    "class_type": "SDPromptReader",
    "_meta": {
      "title": "SD Prompt Reader"
    }
  },
  "130": {
    "inputs": {
      "ckpt_name": "juggernautXL_v9Rdphoto2Lightning.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint Base"
    }
  },
  "131": {
    "inputs": {
      "text": "anime, studio ghibli, ghibli style, a cute [girl], smile, split solid color background, portrait, close up, f1.2",
      "clip": [
        "144",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Positive Prompt)"
    }
  },
  "132": {
    "inputs": {
      "text": "Signature, deformed, smooth, plastic, blurry, grainy, ((hands)), ((hand)), photo\n",
      "clip": [
        "144",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Negative Prompt)"
    }
  },
  "133": {
    "inputs": {
      "weight": 0.6,
      "start_at": 0,
      "end_at": 1,
      "instantid": [
        "134",
        0
      ],
      "insightface": [
        "135",
        0
      ],
      "control_net": [
        "136",
        0
      ],
      "image": [
        "137",
        0
      ],
      "model": [
        "144",
        0
      ],
      "positive": [
        "131",
        0
      ],
      "negative": [
        "132",
        0
      ],
      "image_kps": [
        "139",
        0
      ]
    },
    "class_type": "ApplyInstantID",
    "_meta": {
      "title": "Apply InstantID"
    }
  },
  "134": {
    "inputs": {
      "instantid_file": "ip-adapter.bin"
    },
    "class_type": "InstantIDModelLoader",
    "_meta": {
      "title": "Load InstantID Model"
    }
  },
  "135": {
    "inputs": {
      "provider": "CPU"
    },
    "class_type": "InstantIDFaceAnalysis",
    "_meta": {
      "title": "InstantID Face Analysis"
    }
  },
  "136": {
    "inputs": {
      "control_net_name": "instantid\\diffusion_pytorch_model.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "137": {
    "inputs": {
      "image": "16481962861_640x426 (2).jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "face"
    }
  },
  "138": {
    "inputs": {
      "width": 832,
      "height": 1216,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "139": {
    "inputs": {
      "image": "pose.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "pose"
    }
  },
  "140": {
    "inputs": {
      "seed": 1088405562001119,
      "steps": 4,
      "cfg": 2.5,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 1,
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "145",
        0
      ],
      "positive": [
        "133",
        1
      ],
      "negative": [
        "133",
        2
      ],
      "latent_image": [
        "138",
        0
      ],
      "optional_vae": [
        "130",
        2
      ],
      "script": [
        "141",
        0
      ]
    },
    "class_type": "KSampler (Efficient)",
    "_meta": {
      "title": "KSampler (Efficient)"
    }
  },
  "141": {
    "inputs": {
      "upscale_type": "latent",
      "hires_ckpt_name": "(use same)",
      "latent_upscaler": "ttl_nn.SDXL",
      "pixel_upscaler": "4x_NMKD-Siax_200k.pth",
      "upscale_by": 1.5,
      "use_same_seed": true,
      "seed": -1,
      "hires_steps": 4,
      "denoise": 0.5,
      "iterations": 1,
      "use_controlnet": false,
      "control_net_name": "depth-zoe-xl-v1.0-controlnet.safetensors",
      "strength": 0.8,
      "preprocessor": "DepthAnythingPreprocessor",
      "preprocessor_imgs": true
    },
    "class_type": "HighRes-Fix Script",
    "_meta": {
      "title": "HighRes-Fix Script"
    }
  },
  "142": {
    "inputs": {
      "filename_prefix": "Ghibli Portrait Studio/img_",
      "images": [
        "140",
        5
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "@Datou https://twitter.com/Datou"
    }
  },
  "144": {
    "inputs": {
      "lora_name": "StudioGhibli.Redmond-StdGBRRedmAF-StudioGhibli.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "130",
        0
      ],
      "clip": [
        "130",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "145": {
    "inputs": {
      "hard_mode": true,
      "boost": true,
      "model": [
        "133",
        0
      ]
    },
    "class_type": "Automatic CFG",
    "_meta": {
      "title": "Automatic CFG"
    }
  },
  "147": {
    "inputs": {
      "ckpt_name": "realvisxlV40_v40LightningBakedvae.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "148": {
    "inputs": {
      "text": "city street, neon, fog, volumetric, closeup portrait photo of young woman in dark clothes\n",
      "clip": [
        "154",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "149": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:verybadimagenegative_v1.3, embedding:bad-picture-chill-75v, (worst quality, low quality, illustration, 3d, 2d, painting, cartoons, sketch), open mouth",
      "clip": [
        "154",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "150": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "151": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 978166729551121,
      "steps": 8,
      "cfg": 1,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "152",
        0
      ],
      "positive": [
        "148",
        0
      ],
      "negative": [
        "149",
        0
      ],
      "latent_image": [
        "150",
        0
      ],
      "optional_vae": [
        "147",
        2
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "152": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "147",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "153": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "151",
        5
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "154": {
    "inputs": {
      "stop_at_clip_layer": -2,
      "clip": [
        "147",
        1
      ]
    },
    "class_type": "CLIPSetLastLayer",
    "_meta": {
      "title": "CLIP Set Last Layer"
    }
  },
  "156": {
    "inputs": {
      "image": "287754011_352722980317506_4882317591062679872_n.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "157": {
    "inputs": {
      "rgthree_comparer": {
        "images": [
          {
            "name": "A",
            "selected": true,
            "url": "/view?filename=rgthree.compare._temp_fpfsy_00017_.png&type=temp&subfolder=&rand=0.44239281322071267"
          },
          {
            "name": "B",
            "selected": true,
            "url": "/view?filename=rgthree.compare._temp_fpfsy_00018_.png&type=temp&subfolder=&rand=0.8742168790675953"
          }
        ]
      },
      "image_a": [
        "158",
        0
      ],
      "image_b": [
        "156",
        0
      ]
    },
    "class_type": "Image Comparer (rgthree)",
    "_meta": {
      "title": "Image Comparer (rgthree)"
    }
  },
  "158": {
    "inputs": {
      "upscale_by": 2,
      "seed": 114732666620946,
      "steps": 8,
      "cfg": 1,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 1024,
      "tile_height": 1024,
      "mask_blur": 12,
      "tile_padding": 64,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "156",
        0
      ],
      "model": [
        "163",
        0
      ],
      "positive": [
        "161",
        0
      ],
      "negative": [
        "162",
        0
      ],
      "vae": [
        "163",
        2
      ],
      "upscale_model": [
        "159",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "159": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "160": {
    "inputs": {
      "model": "wd-v1-4-moat-tagger-v2",
      "threshold": 0.35,
      "character_threshold": 0.85,
      "replace_underscore": false,
      "trailing_comma": false,
      "exclude_tags": "",
      "image": [
        "156",
        0
      ]
    },
    "class_type": "WD14Tagger|pysssss",
    "_meta": {
      "title": "WD14 Tagger 🐍"
    }
  },
  "161": {
    "inputs": {
      "text": [
        "160",
        0
      ],
      "clip": [
        "163",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "162": {
    "inputs": {
      "text": "(octane render, render, drawing, anime, bad photo, bad photography:1.3), (worst quality, low quality, blurry:1.2), (bad teeth, deformed teeth, deformed lips), (bad anatomy, bad proportions:1.1), (deformed iris, deformed pupils), (deformed eyes, bad eyes), (deformed face, ugly face, bad face), (deformed hands, bad hands, fused fingers), morbid, mutilated, mutation, disfigured\n",
      "clip": [
        "163",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "163": {
    "inputs": {
      "ckpt_name": "realvisxlV40_v40LightningBakedvae.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "165": {
    "inputs": {
      "ckpt_name": "xxmixunreal_v10.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "166": {
    "inputs": {
      "text": "1girl, curly hair, purple hair",
      "clip": [
        "165",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "167": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:verybadimagenegative_v1.3, embedding:bad-picture-chill-75v, ",
      "clip": [
        "165",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "168": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "169": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "170": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 1023384462677975,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "180",
        0
      ],
      "positive": [
        "166",
        0
      ],
      "negative": [
        "167",
        0
      ],
      "latent_image": [
        "169",
        0
      ],
      "optional_vae": [
        "168",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "171": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "165",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "172": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 589412165528809,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "euler_ancestral",
      "scheduler": "normal",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "170",
        5
      ],
      "model": [
        "170",
        0
      ],
      "positive": [
        "170",
        1
      ],
      "negative": [
        "170",
        2
      ],
      "vae": [
        "170",
        4
      ],
      "upscale_model": [
        "173",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "173": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "174": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  },
  "175": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "176": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "172",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "177": {
    "inputs": {
      "image": "022e08eb-c7d3-41ca-b53c-1905d7210fbe.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "178": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "179": {
    "inputs": {
      "ipadapter_file": "ip-adapter-plus_sd15.safetensors"
    },
    "class_type": "MZ_IPAdapterModelLoaderKolors",
    "_meta": {
      "title": "IPAdapterModelLoader(kolors) - Legacy"
    }
  },
  "180": {
    "inputs": {
      "weight": 1,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "171",
        0
      ],
      "ipadapter": [
        "179",
        0
      ],
      "image": [
        "177",
        0
      ],
      "clip_vision": [
        "178",
        0
      ]
    },
    "class_type": "MZ_IPAdapterAdvancedKolors",
    "_meta": {
      "title": "IPAdapterAdvanced(kolors) - Legacy"
    }
  },
  "182": {
    "inputs": {
      "weight": 0.75,
      "projection": "ortho_v2",
      "fidelity": 8,
      "noise": 0,
      "start_at": 0,
      "end_at": 1,
      "model": [
        "197",
        0
      ],
      "pulid": [
        "184",
        0
      ],
      "eva_clip": [
        "185",
        0
      ],
      "face_analysis": [
        "183",
        0
      ],
      "image": [
        "196",
        0
      ]
    },
    "class_type": "ApplyPulidAdvanced",
    "_meta": {
      "title": "Apply PuLID Advanced"
    }
  },
  "183": {
    "inputs": {
      "provider": "CPU"
    },
    "class_type": "PulidInsightFaceLoader",
    "_meta": {
      "title": "Load InsightFace (PuLID)"
    }
  },
  "184": {
    "inputs": {
      "pulid_file": "ip-adapter_pulid_sdxl_fp16.safetensors"
    },
    "class_type": "PulidModelLoader",
    "_meta": {
      "title": "Load PuLID Model"
    }
  },
  "185": {
    "inputs": {},
    "class_type": "PulidEvaClipLoader",
    "_meta": {
      "title": "Load Eva Clip (PuLID)"
    }
  },
  "186": {
    "inputs": {
      "ckpt_name": "dreamshaperXL_lightningDPMSDE.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "187": {
    "inputs": {
      "image": "306759611_198561015933063_523199936358037512_n.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "188": {
    "inputs": {
      "text": "1girl, happy, Sticker, svg, vector art, sharp, kawaii style, Anime style",
      "clip": [
        "197",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "189": {
    "inputs": {
      "text": "",
      "clip": [
        "197",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "190": {
    "inputs": {
      "seed": 944904466250849,
      "steps": 4,
      "cfg": 1.5,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 1,
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "191",
        0
      ],
      "positive": [
        "188",
        0
      ],
      "negative": [
        "189",
        0
      ],
      "latent_image": [
        "192",
        0
      ],
      "optional_vae": [
        "186",
        2
      ]
    },
    "class_type": "KSampler (Efficient)",
    "_meta": {
      "title": "KSampler (Efficient)"
    }
  },
  "191": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "182",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "192": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "193": {
    "inputs": {
      "images": [
        "199",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "194": {
    "inputs": {
      "image": "Phạm Như.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "195": {
    "inputs": {
      "image": "287754011_352722980317506_4882317591062679872_n.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "196": {
    "inputs": {
      "method": "lanczos",
      "image_1": [
        "187",
        0
      ],
      "image_2": [
        "194",
        0
      ],
      "image_3": [
        "195",
        0
      ]
    },
    "class_type": "ImageBatchMultiple+",
    "_meta": {
      "title": "🔧 Images Batch Multiple"
    }
  },
  "197": {
    "inputs": {
      "lora_name": "StickersRedmond.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "186",
        0
      ],
      "clip": [
        "186",
        1
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "198": {
    "inputs": {},
    "class_type": "BRIA_RMBG_ModelLoader_Zho",
    "_meta": {
      "title": "🧹BRIA_RMBG Model Loader"
    }
  },
  "199": {
    "inputs": {
      "rmbgmodel": [
        "198",
        0
      ],
      "image": [
        "190",
        5
      ]
    },
    "class_type": "BRIA_RMBG_Zho",
    "_meta": {
      "title": "🧹BRIA RMBG"
    }
  },
  "201": {
    "inputs": {
      "image": "cach-chup-hinh-the-dep.jpeg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "202": {
    "inputs": {
      "ckpt_name": "juggernautXL_v9Rdphoto2Lightning.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "203": {
    "inputs": {
      "seed": 10,
      "steps": 7,
      "cfg": 2,
      "sampler_name": "dpmpp_sde",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "211",
        0
      ],
      "positive": [
        "222",
        0
      ],
      "negative": [
        "222",
        1
      ],
      "latent_image": [
        "222",
        3
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "204": {
    "inputs": {
      "samples": [
        "203",
        0
      ],
      "vae": [
        "202",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "205": {
    "inputs": {
      "text": "",
      "clip": [
        "202",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "206": {
    "inputs": {
      "text": "",
      "clip": [
        "202",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "211": {
    "inputs": {
      "model": [
        "219",
        0
      ]
    },
    "class_type": "DifferentialDiffusion",
    "_meta": {
      "title": "Differential Diffusion"
    }
  },
  "212": {
    "inputs": {
      "image": "$212-0",
      "images": [
        "204",
        0
      ]
    },
    "class_type": "PreviewBridge",
    "_meta": {
      "title": "Preview Bridge (Image)"
    }
  },
  "214": {
    "inputs": {
      "rgthree_comparer": {
        "images": [
          {
            "name": "A",
            "selected": true,
            "url": "/view?filename=rgthree.compare._temp_omqmj_00001_.png&type=temp&subfolder=&rand=0.8723850379867464"
          },
          {
            "name": "B",
            "selected": true,
            "url": "/view?filename=rgthree.compare._temp_omqmj_00002_.png&type=temp&subfolder=&rand=0.49114573096007796"
          }
        ]
      },
      "image_a": [
        "230",
        0
      ],
      "image_b": [
        "217",
        0
      ]
    },
    "class_type": "Image Comparer (rgthree)",
    "_meta": {
      "title": "Image Comparer (rgthree)"
    }
  },
  "216": {
    "inputs": {
      "images": [
        "217",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "217": {
    "inputs": {
      "left": 0,
      "top": 0,
      "right": 0,
      "bottom": 128,
      "feathering": 60,
      "image": [
        "223",
        0
      ]
    },
    "class_type": "ImagePadForOutpaintMasked",
    "_meta": {
      "title": "Image Pad For Outpaint Masked"
    }
  },
  "218": {
    "inputs": {
      "mask": [
        "217",
        1
      ]
    },
    "class_type": "MaskPreview+",
    "_meta": {
      "title": "🔧 Mask Preview"
    }
  },
  "219": {
    "inputs": {
      "model": [
        "202",
        0
      ],
      "patch": [
        "220",
        0
      ],
      "latent": [
        "222",
        2
      ]
    },
    "class_type": "INPAINT_ApplyFooocusInpaint",
    "_meta": {
      "title": "Apply Fooocus Inpaint"
    }
  },
  "220": {
    "inputs": {
      "head": "fooocus_inpaint_head.pth",
      "patch": "inpaint_v26.fooocus.patch"
    },
    "class_type": "INPAINT_LoadFooocusInpaint",
    "_meta": {
      "title": "Load Fooocus Inpaint"
    }
  },
  "221": {
    "inputs": {
      "fill": "telea",
      "falloff": 0,
      "image": [
        "217",
        0
      ],
      "mask": [
        "217",
        1
      ]
    },
    "class_type": "INPAINT_MaskedFill",
    "_meta": {
      "title": "Fill Masked Area"
    }
  },
  "222": {
    "inputs": {
      "positive": [
        "205",
        0
      ],
      "negative": [
        "206",
        0
      ],
      "vae": [
        "202",
        2
      ],
      "pixels": [
        "228",
        0
      ],
      "mask": [
        "217",
        1
      ]
    },
    "class_type": "INPAINT_VAEEncodeInpaintConditioning",
    "_meta": {
      "title": "VAE Encode & Inpaint Conditioning"
    }
  },
  "223": {
    "inputs": {
      "width": 1216,
      "height": 1216,
      "scale_option": "width",
      "average_color": "on",
      "fill_color": "#FFFFFF",
      "image": [
        "201",
        0
      ]
    },
    "class_type": "ResizeImageMixlab",
    "_meta": {
      "title": "Resize Image ♾️Mixlab"
    }
  },
  "224": {
    "inputs": {
      "image": [
        "223",
        0
      ]
    },
    "class_type": "ImageSizeAndBatchSize",
    "_meta": {
      "title": "Get Image Size + Batch Size"
    }
  },
  "225": {
    "inputs": {
      "output": "",
      "source": [
        "224",
        1
      ]
    },
    "class_type": "Display Any (rgthree)",
    "_meta": {
      "title": "Display Any (rgthree)"
    }
  },
  "226": {
    "inputs": {
      "output": "",
      "source": [
        "224",
        0
      ]
    },
    "class_type": "Display Any (rgthree)",
    "_meta": {
      "title": "Display Any (rgthree)"
    }
  },
  "227": {
    "inputs": {
      "images": [
        "221",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "228": {
    "inputs": {
      "blur": 40,
      "falloff": 0,
      "image": [
        "221",
        0
      ],
      "mask": [
        "217",
        1
      ]
    },
    "class_type": "INPAINT_MaskedBlur",
    "_meta": {
      "title": "Blur Masked Area"
    }
  },
  "229": {
    "inputs": {
      "images": [
        "228",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "230": {
    "inputs": {
      "x": 0,
      "y": 0,
      "resize_source": false,
      "destination": [
        "217",
        0
      ],
      "source": [
        "212",
        0
      ],
      "mask": [
        "217",
        1
      ]
    },
    "class_type": "ImageCompositeMasked",
    "_meta": {
      "title": "ImageCompositeMasked"
    }
  },
  "231": {
    "inputs": {
      "image": [
        "201",
        0
      ]
    },
    "class_type": "ImageSizeAndBatchSize",
    "_meta": {
      "title": "Get Image Size + Batch Size"
    }
  },
  "232": {
    "inputs": {
      "output": "",
      "source": [
        "231",
        1
      ]
    },
    "class_type": "Display Any (rgthree)",
    "_meta": {
      "title": "Display Any (rgthree)"
    }
  },
  "233": {
    "inputs": {
      "output": "",
      "source": [
        "231",
        0
      ]
    },
    "class_type": "Display Any (rgthree)",
    "_meta": {
      "title": "Display Any (rgthree)"
    }
  },
  "234": {
    "inputs": {
      "samples": [
        "203",
        0
      ]
    },
    "class_type": "RemoveNoiseMask",
    "_meta": {
      "title": "Remove Noise Mask"
    }
  },
  "236": {
    "inputs": {
      "weight": 1,
      "projection": "ortho_v2",
      "fidelity": 8,
      "noise": 0,
      "start_at": 0,
      "end_at": 1,
      "model": [
        "251",
        0
      ],
      "pulid": [
        "238",
        0
      ],
      "eva_clip": [
        "239",
        0
      ],
      "face_analysis": [
        "237",
        0
      ],
      "image": [
        "250",
        0
      ]
    },
    "class_type": "ApplyPulidAdvanced",
    "_meta": {
      "title": "Apply PuLID Advanced"
    }
  },
  "237": {
    "inputs": {
      "provider": "CPU"
    },
    "class_type": "PulidInsightFaceLoader",
    "_meta": {
      "title": "Load InsightFace (PuLID)"
    }
  },
  "238": {
    "inputs": {
      "pulid_file": "ip-adapter_pulid_sdxl_fp16.safetensors"
    },
    "class_type": "PulidModelLoader",
    "_meta": {
      "title": "Load PuLID Model"
    }
  },
  "239": {
    "inputs": {},
    "class_type": "PulidEvaClipLoader",
    "_meta": {
      "title": "Load Eva Clip (PuLID)"
    }
  },
  "240": {
    "inputs": {
      "ckpt_name": "dreamshaperXL_lightningDPMSDE.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "241": {
    "inputs": {
      "image": "306759611_198561015933063_523199936358037512_n.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "242": {
    "inputs": {
      "text": "1girl, happy, cute girl, svg, vector art, sharp, cute style, 3DRenderAF, 3D Render Style",
      "clip": [
        "251",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "243": {
    "inputs": {
      "text": "(worst quality, low quality, open mouth",
      "clip": [
        "251",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "244": {
    "inputs": {
      "seed": 651345377995249,
      "steps": 8,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "sgm_uniform",
      "denoise": 1,
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "253",
        0
      ],
      "positive": [
        "242",
        0
      ],
      "negative": [
        "243",
        0
      ],
      "latent_image": [
        "246",
        0
      ],
      "optional_vae": [
        "240",
        2
      ],
      "script": [
        "254",
        0
      ]
    },
    "class_type": "KSampler (Efficient)",
    "_meta": {
      "title": "KSampler (Efficient)"
    }
  },
  "245": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "236",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "246": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "247": {
    "inputs": {
      "images": [
        "244",
        5
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "248": {
    "inputs": {
      "image": "Phạm Như.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "249": {
    "inputs": {
      "image": "287754011_352722980317506_4882317591062679872_n.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "250": {
    "inputs": {
      "method": "lanczos",
      "image_1": [
        "241",
        0
      ],
      "image_2": [
        "248",
        0
      ],
      "image_3": [
        "249",
        0
      ]
    },
    "class_type": "ImageBatchMultiple+",
    "_meta": {
      "title": "🔧 Images Batch Multiple"
    }
  },
  "251": {
    "inputs": {
      "lora_name": "3DRedmond-3DRenderStyle-3DRenderAF.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "240",
        0
      ],
      "clip": [
        "252",
        0
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "252": {
    "inputs": {
      "stop_at_clip_layer": -2,
      "clip": [
        "240",
        1
      ]
    },
    "class_type": "CLIPSetLastLayer",
    "_meta": {
      "title": "CLIP Set Last Layer"
    }
  },
  "253": {
    "inputs": {
      "hard_mode": true,
      "boost": true,
      "model": [
        "245",
        0
      ]
    },
    "class_type": "Automatic CFG",
    "_meta": {
      "title": "Automatic CFG"
    }
  },
  "254": {
    "inputs": {
      "upscale_type": "latent",
      "hires_ckpt_name": "(use same)",
      "latent_upscaler": "ttl_nn.SDXL",
      "pixel_upscaler": "4x_NMKD-Siax_200k.pth",
      "upscale_by": 1.5,
      "use_same_seed": true,
      "seed": -1,
      "hires_steps": 4,
      "denoise": 0.5,
      "iterations": 1,
      "use_controlnet": false,
      "control_net_name": "depth-zoe-xl-v1.0-controlnet.safetensors",
      "strength": 0.8,
      "preprocessor": "DepthAnythingPreprocessor",
      "preprocessor_imgs": true
    },
    "class_type": "HighRes-Fix Script",
    "_meta": {
      "title": "HighRes-Fix Script"
    }
  },
  "256": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "257": {
    "inputs": {
      "text": [
        "273",
        0
      ],
      "clip": [
        "289",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "positive"
    }
  },
  "258": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:bad-picture-chill-75v, embedding:bad_pictures, embedding:verybadimagenegative_v1.3, ",
      "clip": [
        "289",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Negative"
    }
  },
  "259": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "263",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "260": {
    "inputs": {
      "ckpt_name": "majicmixRealistic_v6.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "261": {
    "inputs": {
      "CLIP": [
        "289",
        1
      ]
    },
    "class_type": "Anything Everywhere",
    "_meta": {
      "title": "Anything Everywhere"
    }
  },
  "262": {
    "inputs": {
      "VAE": [
        "266",
        0
      ]
    },
    "class_type": "Anything Everywhere",
    "_meta": {
      "title": "Anything Everywhere"
    }
  },
  "263": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 207855210189991,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "265",
        0
      ],
      "model": [
        "289",
        0
      ],
      "clip": [
        "289",
        1
      ],
      "positive": [
        "257",
        0
      ],
      "negative": [
        "258",
        0
      ],
      "bbox_detector": [
        "264",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "264": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "265": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 733350115632987,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 8,
      "tile_padding": 32,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "269",
        0
      ],
      "model": [
        "289",
        0
      ],
      "positive": [
        "257",
        0
      ],
      "negative": [
        "258",
        0
      ],
      "upscale_model": [
        "267",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "266": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_vaeFtMse840000Pt.pt"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "267": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "268": {
    "inputs": {
      "seed": 1119173688572084,
      "steps": 30,
      "cfg": 5,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "287",
        0
      ],
      "positive": [
        "257",
        0
      ],
      "negative": [
        "258",
        0
      ],
      "latent_image": [
        "256",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "269": {
    "inputs": {
      "samples": [
        "268",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "270": {
    "inputs": {
      "images": [
        "269",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "273": {
    "inputs": {
      "text": "1girl,character portrait,cinematic lighting,drop shadow, (full body:0.9), ponytail, simple background, white background"
    },
    "class_type": "Text _O",
    "_meta": {
      "title": "Text _O"
    }
  },
  "278": {
    "inputs": {
      "image": "vecteezy_woman-dress-isolated-on-transparent-background_45801005.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "279": {
    "inputs": {
      "weight": 1,
      "ipadapter": [
        "280",
        1
      ],
      "image": [
        "278",
        0
      ]
    },
    "class_type": "IPAdapterEncoder",
    "_meta": {
      "title": "IPAdapter Encoder"
    }
  },
  "280": {
    "inputs": {
      "preset": "PLUS (high strength)",
      "model": [
        "289",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "281": {
    "inputs": {
      "method": "concat",
      "embed1": [
        "279",
        0
      ]
    },
    "class_type": "IPAdapterCombineEmbeds",
    "_meta": {
      "title": "IPAdapter Combine Embeds"
    }
  },
  "282": {
    "inputs": {
      "weight": 1,
      "weight_type": "weak input",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "K+V",
      "model": [
        "280",
        0
      ],
      "ipadapter": [
        "280",
        1
      ],
      "pos_embed": [
        "281",
        0
      ],
      "neg_embed": [
        "284",
        1
      ]
    },
    "class_type": "IPAdapterEmbeds",
    "_meta": {
      "title": "IPAdapter Embeds"
    }
  },
  "283": {
    "inputs": {
      "type": "gaussian",
      "strength": 0.55,
      "blur": 0
    },
    "class_type": "IPAdapterNoise",
    "_meta": {
      "title": "IPAdapter Noise"
    }
  },
  "284": {
    "inputs": {
      "weight": 1,
      "ipadapter": [
        "280",
        1
      ],
      "image": [
        "283",
        0
      ]
    },
    "class_type": "IPAdapterEncoder",
    "_meta": {
      "title": "IPAdapter Encoder"
    }
  },
  "285": {
    "inputs": {
      "b1": 1.3,
      "b2": 1.4,
      "s1": 0.9,
      "s2": 0.2,
      "model": [
        "282",
        0
      ]
    },
    "class_type": "FreeU_V2",
    "_meta": {
      "title": "FreeU_V2"
    }
  },
  "286": {
    "inputs": {
      "scale": 3,
      "model": [
        "285",
        0
      ]
    },
    "class_type": "PerturbedAttentionGuidance",
    "_meta": {
      "title": "PerturbedAttentionGuidance"
    }
  },
  "287": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "286",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "288": {
    "inputs": {
      "stop_at_clip_layer": -2,
      "clip": [
        "260",
        1
      ]
    },
    "class_type": "CLIPSetLastLayer",
    "_meta": {
      "title": "CLIP Set Last Layer"
    }
  },
  "289": {
    "inputs": {
      "lora_name": "gentle girls.safetensors",
      "strength_model": 0.4,
      "strength_clip": 0.4,
      "model": [
        "260",
        0
      ],
      "clip": [
        "288",
        0
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "291": {
    "inputs": {
      "ckpt_name": "realvisxlV40_v40LightningBakedvae.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "292": {
    "inputs": {
      "text": "1 room, bed room, bed, pillow, blanket, curtain, plant pot, hyper realistic, realistic, no human",
      "clip": [
        "291",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "293": {
    "inputs": {
      "text": "logo, text,word,cropped,low quality,normal quality,username,watermark,signature,blurry,soft,soft line,sketch,ugly,logo,pixelated,lowres, girl, man, men, women, woman, human",
      "clip": [
        "291",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "294": {
    "inputs": {
      "width": 512,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "295": {
    "inputs": {
      "add_noise": "enable",
      "noise_seed": 731231467279957,
      "steps": 8,
      "cfg": 1,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "start_at_step": 0,
      "end_at_step": 10000,
      "return_with_leftover_noise": "disable",
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "296",
        0
      ],
      "positive": [
        "292",
        0
      ],
      "negative": [
        "293",
        0
      ],
      "latent_image": [
        "294",
        0
      ],
      "script": [
        "301",
        0
      ]
    },
    "class_type": "KSampler Adv. (Efficient)",
    "_meta": {
      "title": "KSampler Adv. (Efficient)"
    }
  },
  "296": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "298",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "297": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "295",
        5
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "298": {
    "inputs": {
      "lora_name": "JJsBedroom_XL.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "291",
        0
      ],
      "clip": [
        "299",
        0
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "299": {
    "inputs": {
      "stop_at_clip_layer": -2,
      "clip": [
        "291",
        1
      ]
    },
    "class_type": "CLIPSetLastLayer",
    "_meta": {
      "title": "CLIP Set Last Layer"
    }
  },
  "300": {
    "inputs": {
      "VAE": [
        "291",
        2
      ]
    },
    "class_type": "Anything Everywhere",
    "_meta": {
      "title": "Anything Everywhere"
    }
  },
  "301": {
    "inputs": {
      "upscale_type": "latent",
      "hires_ckpt_name": "(use same)",
      "latent_upscaler": "ttl_nn.SDXL",
      "pixel_upscaler": "4x_NMKD-Siax_200k.pth",
      "upscale_by": 1.5,
      "use_same_seed": true,
      "seed": -1,
      "hires_steps": 4,
      "denoise": 0.5,
      "iterations": 1,
      "use_controlnet": false,
      "control_net_name": "depth-zoe-xl-v1.0-controlnet.safetensors",
      "strength": 0.8,
      "preprocessor": "DepthAnythingPreprocessor",
      "preprocessor_imgs": true
    },
    "class_type": "HighRes-Fix Script",
    "_meta": {
      "title": "HighRes-Fix Script"
    }
  },
  "303": {
    "inputs": {
      "ckpt_name": "dreamshaper_8.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "304": {
    "inputs": {
      "text": "headphone, black headphone, beach, ocean, cloud, sun, day",
      "clip": [
        "303",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "305": {
    "inputs": {
      "text": "embedding:badhandv4, embedding:negative_hand-neg, embedding:ng_deepnegative_v1_75t, embedding:bad-picture-chill-75v, embedding:bad_pictures, embedding:verybadimagenegative_v1.3, ",
      "clip": [
        "303",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "306": {
    "inputs": {
      "multiplier": 0.2,
      "positive": [
        "304",
        0
      ],
      "negative": [
        "305",
        0
      ],
      "vae": [
        "303",
        2
      ],
      "foreground": [
        "315",
        0
      ]
    },
    "class_type": "ICLightConditioning",
    "_meta": {
      "title": "IC-Light Conditioning"
    }
  },
  "307": {
    "inputs": {
      "model_path": "iclight_sd15_fc.safetensors",
      "model": [
        "303",
        0
      ]
    },
    "class_type": "LoadAndApplyICLightUnet",
    "_meta": {
      "title": "Load And Apply IC-Light"
    }
  },
  "308": {
    "inputs": {
      "image": "86c1ac8bac43ea337f7fe9da5c87a7fd.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "309": {
    "inputs": {
      "width": 1024,
      "height": 1024,
      "interpolation": "nearest",
      "method": "keep proportion",
      "condition": "always",
      "multiple_of": 0,
      "image": [
        "321",
        0
      ]
    },
    "class_type": "ImageResize+",
    "_meta": {
      "title": "🔧 Image Resize"
    }
  },
  "310": {
    "inputs": {
      "shape": "circle",
      "frames": 1,
      "location_x": 256,
      "location_y": 256,
      "grow": 17,
      "frame_width": [
        "309",
        1
      ],
      "frame_height": [
        "309",
        2
      ],
      "shape_width": 256,
      "shape_height": 256
    },
    "class_type": "CreateShapeMask",
    "_meta": {
      "title": "Create Shape Mask"
    }
  },
  "311": {
    "inputs": {
      "expand": 0,
      "incremental_expandrate": 0,
      "tapered_corners": true,
      "flip_input": false,
      "blur_radius": 0,
      "lerp_alpha": 1,
      "decay_factor": 1,
      "fill_holes": false,
      "mask": [
        "310",
        0
      ]
    },
    "class_type": "GrowMaskWithBlur",
    "_meta": {
      "title": "Grow Mask With Blur"
    }
  },
  "312": {
    "inputs": {
      "min": 0,
      "max": 1,
      "mask": [
        "311",
        0
      ]
    },
    "class_type": "RemapMaskRange",
    "_meta": {
      "title": "Remap Mask Range"
    }
  },
  "313": {
    "inputs": {
      "mask": [
        "312",
        0
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "314": {
    "inputs": {
      "pixels": [
        "313",
        0
      ],
      "vae": [
        "303",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "315": {
    "inputs": {
      "pixels": [
        "309",
        0
      ],
      "vae": [
        "303",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "316": {
    "inputs": {
      "upscale_by": 1.5,
      "seed": 947655511812880,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "dpmpp_2m_sde",
      "scheduler": "karras",
      "denoise": 0.2,
      "mode_type": "Linear",
      "tile_width": 512,
      "tile_height": 512,
      "mask_blur": 12,
      "tile_padding": 64,
      "seam_fix_mode": "None",
      "seam_fix_denoise": 1,
      "seam_fix_width": 64,
      "seam_fix_mask_blur": 8,
      "seam_fix_padding": 16,
      "force_uniform_tiles": true,
      "tiled_decode": false,
      "image": [
        "324",
        0
      ],
      "model": [
        "319",
        0
      ],
      "positive": [
        "306",
        0
      ],
      "negative": [
        "306",
        1
      ],
      "vae": [
        "303",
        2
      ],
      "upscale_model": [
        "317",
        0
      ]
    },
    "class_type": "UltimateSDUpscale",
    "_meta": {
      "title": "Ultimate SD Upscale"
    }
  },
  "317": {
    "inputs": {
      "model_name": "4x-UltraSharp.pth"
    },
    "class_type": "UpscaleModelLoader",
    "_meta": {
      "title": "Load Upscale Model"
    }
  },
  "318": {
    "inputs": {
      "images": [
        "316",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "319": {
    "inputs": {
      "method": "Mixture of Diffusers",
      "tile_width": 768,
      "tile_height": 768,
      "tile_overlap": 64,
      "tile_batch_size": 4,
      "model": [
        "307",
        0
      ]
    },
    "class_type": "TiledDiffusion",
    "_meta": {
      "title": "Tiled Diffusion"
    }
  },
  "320": {
    "inputs": {
      "model": "u2netp: lightweight general purpose",
      "providers": "CPU"
    },
    "class_type": "RemBGSession+",
    "_meta": {
      "title": "🔧 RemBG Session"
    }
  },
  "321": {
    "inputs": {
      "rembg_session": [
        "320",
        0
      ],
      "image": [
        "308",
        0
      ]
    },
    "class_type": "ImageRemoveBackground+",
    "_meta": {
      "title": "🔧 Image Remove Background"
    }
  },
  "322": {
    "inputs": {
      "images": [
        "321",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "323": {
    "inputs": {
      "seed": 810455001533175,
      "steps": 30,
      "cfg": 5,
      "sampler_name": "dpmpp_2m_sde",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "307",
        0
      ],
      "positive": [
        "306",
        0
      ],
      "negative": [
        "306",
        1
      ],
      "latent_image": [
        "314",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "324": {
    "inputs": {
      "samples": [
        "323",
        0
      ],
      "vae": [
        "303",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "331": {
    "inputs": {
      "image": "306759611_198561015933063_523199936358037512_n.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "332": {
    "inputs": {
      "images": [
        "333",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "333": {
    "inputs": {
      "color": "#000000",
      "background": "#000000",
      "mask": [
        "334",
        0
      ]
    },
    "class_type": "Mask To Image (mtb)",
    "_meta": {
      "title": "Mask To Image (mtb)"
    }
  },
  "334": {
    "inputs": {
      "value": 1,
      "width": [
        "335",
        0
      ],
      "height": [
        "335",
        1
      ]
    },
    "class_type": "SolidMask",
    "_meta": {
      "title": "SolidMask"
    }
  },
  "335": {
    "inputs": {
      "image": [
        "331",
        0
      ]
    },
    "class_type": "GetImageSize+",
    "_meta": {
      "title": "🔧 Get Image Size"
    }
  },
  "336": {
    "inputs": {
      "prompt": "clothes",
      "threshold": 0.3,
      "sam_model": [
        "337",
        0
      ],
      "grounding_dino_model": [
        "338",
        0
      ],
      "image": [
        "331",
        0
      ]
    },
    "class_type": "GroundingDinoSAMSegment (segment anything)",
    "_meta": {
      "title": "GroundingDinoSAMSegment (segment anything)"
    }
  },
  "337": {
    "inputs": {
      "model_name": "sam_hq_vit_h (2.57GB)"
    },
    "class_type": "SAMModelLoader (segment anything)",
    "_meta": {
      "title": "SAMModelLoader (segment anything)"
    }
  },
  "338": {
    "inputs": {
      "model_name": "GroundingDINO_SwinB (938MB)"
    },
    "class_type": "GroundingDinoModelLoader (segment anything)",
    "_meta": {
      "title": "GroundingDinoModelLoader (segment anything)"
    }
  },
  "339": {
    "inputs": {
      "images": [
        "336",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "340": {
    "inputs": {
      "mask": [
        "336",
        1
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "341": {
    "inputs": {
      "images": [
        "340",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "342": {
    "inputs": {
      "overlay_resize": "None",
      "resize_method": "nearest-exact",
      "rescale_factor": 1,
      "width": 512,
      "height": 512,
      "x_offset": 0,
      "y_offset": 0,
      "rotation": 0,
      "opacity": 0,
      "base_image": [
        "331",
        0
      ],
      "overlay_image": [
        "333",
        0
      ],
      "optional_mask": [
        "344",
        0
      ]
    },
    "class_type": "Image Overlay",
    "_meta": {
      "title": "Image Overlay"
    }
  },
  "343": {
    "inputs": {
      "images": [
        "342",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "344": {
    "inputs": {
      "mask": [
        "377",
        0
      ]
    },
    "class_type": "InvertMask",
    "_meta": {
      "title": "InvertMask"
    }
  },
  "345": {
    "inputs": {
      "seed": 880288241718410,
      "steps": 10,
      "cfg": 7,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 0.58,
      "model": [
        "350",
        0
      ],
      "positive": [
        "367",
        0
      ],
      "negative": [
        "367",
        1
      ],
      "latent_image": [
        "348",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "346": {
    "inputs": {
      "pixels": [
        "383",
        7
      ],
      "vae": [
        "356",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "347": {
    "inputs": {
      "samples": [
        "345",
        0
      ],
      "vae": [
        "356",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "348": {
    "inputs": {
      "samples": [
        "346",
        0
      ],
      "mask": [
        "363",
        0
      ]
    },
    "class_type": "SetLatentNoiseMask",
    "_meta": {
      "title": "Set Latent Noise Mask"
    }
  },
  "349": {
    "inputs": {
      "mask": [
        "364",
        0
      ]
    },
    "class_type": "MaskToImage",
    "_meta": {
      "title": "Convert Mask to Image"
    }
  },
  "350": {
    "inputs": {
      "ckpt_name": "majicmixRealistic_v7.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "351": {
    "inputs": {
      "text": "black clothes,",
      "clip": [
        "353",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "352": {
    "inputs": {
      "text": "(worst quality, low quality: 1.1) embedding:EasyNegativeV2, embedding:negative_hand-neg,",
      "clip": [
        "353",
        0
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "353": {
    "inputs": {
      "stop_at_clip_layer": -2,
      "clip": [
        "350",
        1
      ]
    },
    "class_type": "CLIPSetLastLayer",
    "_meta": {
      "title": "CLIP Set Last Layer"
    }
  },
  "356": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "359": {
    "inputs": {
      "x": 0,
      "y": 0,
      "resize_source": false,
      "destination": [
        "382",
        7
      ],
      "source": [
        "347",
        0
      ],
      "mask": [
        "363",
        0
      ]
    },
    "class_type": "ImageCompositeMasked",
    "_meta": {
      "title": "ImageCompositeMasked"
    }
  },
  "361": {
    "inputs": {
      "filename_prefix": "2024-08-01/20240801212447inpaint",
      "images": [
        "359",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "362": {
    "inputs": {
      "blur_radius": 10,
      "sigma": 1,
      "image": [
        "349",
        0
      ]
    },
    "class_type": "ImageBlur",
    "_meta": {
      "title": "ImageBlur"
    }
  },
  "363": {
    "inputs": {
      "channel": "red",
      "image": [
        "362",
        0
      ]
    },
    "class_type": "ImageToMask",
    "_meta": {
      "title": "Convert Image to Mask"
    }
  },
  "364": {
    "inputs": {
      "expand": 8,
      "tapered_corners": true,
      "mask": [
        "381",
        21
      ]
    },
    "class_type": "GrowMask",
    "_meta": {
      "title": "GrowMask"
    }
  },
  "365": {
    "inputs": {
      "resolution": 512,
      "image": [
        "331",
        0
      ]
    },
    "class_type": "AnimeLineArtPreprocessor",
    "_meta": {
      "title": "Anime Lineart"
    }
  },
  "366": {
    "inputs": {
      "images": [
        "365",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "367": {
    "inputs": {
      "strength": 1,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "351",
        0
      ],
      "negative": [
        "352",
        0
      ],
      "control_net": [
        "368",
        0
      ],
      "image": [
        "365",
        0
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet (Advanced)"
    }
  },
  "368": {
    "inputs": {
      "control_net_name": "control_v11p_sd15_lineart_fp16.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model (lineart)"
    }
  },
  "369": {
    "inputs": {
      "blend_factor": 1,
      "blend_mode": "screen",
      "image1": [
        "370",
        0
      ],
      "image2": [
        "371",
        0
      ]
    },
    "class_type": "ImageBlend",
    "_meta": {
      "title": "ImageBlend"
    }
  },
  "370": {
    "inputs": {
      "batch_image_number": 0,
      "images_batch": [
        "340",
        0
      ]
    },
    "class_type": "Tensor Batch to Image",
    "_meta": {
      "title": "Tensor Batch to Image"
    }
  },
  "371": {
    "inputs": {
      "batch_image_number": 1,
      "images_batch": [
        "340",
        0
      ]
    },
    "class_type": "Tensor Batch to Image",
    "_meta": {
      "title": "Tensor Batch to Image"
    }
  },
  "372": {
    "inputs": {
      "images": [
        "369",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "373": {
    "inputs": {
      "channel": "red",
      "image": [
        "369",
        0
      ]
    },
    "class_type": "ImageToMask",
    "_meta": {
      "title": "Convert Image to Mask"
    }
  },
  "375": {
    "inputs": {
      "blend_factor": 0.3,
      "blend_mode": "normal",
      "image1": [
        "342",
        0
      ],
      "image2": [
        "331",
        0
      ]
    },
    "class_type": "ImageBlend",
    "_meta": {
      "title": "ImageBlend"
    }
  },
  "376": {
    "inputs": {
      "images": [
        "375",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "377": {
    "inputs": {
      "any_01": [
        "379",
        21
      ]
    },
    "class_type": "Any Switch (rgthree)",
    "_meta": {
      "title": "Any Switch (rgthree)"
    }
  },
  "379": {
    "inputs": {
      "mask": [
        "373",
        0
      ]
    },
    "class_type": "Context Big (rgthree)",
    "_meta": {
      "title": "segment anything"
    }
  },
  "381": {
    "inputs": {
      "mask": [
        "377",
        0
      ]
    },
    "class_type": "Context Big (rgthree)",
    "_meta": {
      "title": "Mask"
    }
  },
  "382": {
    "inputs": {
      "images": [
        "342",
        0
      ]
    },
    "class_type": "Context Big (rgthree)",
    "_meta": {
      "title": "Image"
    }
  },
  "383": {
    "inputs": {
      "images": [
        "375",
        0
      ]
    },
    "class_type": "Context Big (rgthree)",
    "_meta": {
      "title": "Context Big (rgthree)"
    }
  },
  "388": {
    "inputs": {
      "width": 576,
      "height": 768,
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "389": {
    "inputs": {
      "text": "1girl,character portrait,cinematic lighting,drop shadow, (full body:0.5), ponytail, simple background, white background, looking at viewer, smile, hidden hand, shirt, white skirt"
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "positive"
    }
  },
  "390": {
    "inputs": {
      "text": "embedding:ng_deepnegative_v1_75t, embedding:badhandv4, embedding:negative_hand-neg, embedding:bad-picture-chill-75v, embedding:verybadimagenegative_v1.3, "
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "Negative"
    }
  },
  "391": {
    "inputs": {
      "enable_feature_guidance": true,
      "feature_guidance_scale": 2.5,
      "sourceModel": [
        "392",
        0
      ],
      "magicClothingModel": [
        "392",
        1
      ],
      "feature_image": [
        "399",
        0
      ]
    },
    "class_type": "Add Magic Clothing Attention",
    "_meta": {
      "title": "Add Magic Clothing Attention"
    }
  },
  "392": {
    "inputs": {
      "magicClothingUnet": "oms_diffusion_768_200000.safetensors",
      "sourceModel": [
        "393",
        0
      ]
    },
    "class_type": "Load Magic Clothing Model",
    "_meta": {
      "title": "Load Magic Clothing Model"
    }
  },
  "393": {
    "inputs": {
      "ckpt_name": "Extreme Krypton Realistic MAX - White White Sauce_V6 branch cut version.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "394": {
    "inputs": {
      "CLIP": [
        "404",
        0
      ]
    },
    "class_type": "Anything Everywhere",
    "_meta": {
      "title": "Anything Everywhere"
    }
  },
  "395": {
    "inputs": {
      "VAE": [
        "411",
        0
      ]
    },
    "class_type": "Anything Everywhere",
    "_meta": {
      "title": "Anything Everywhere"
    }
  },
  "396": {
    "inputs": {
      "image": "70121-ao-thun-nu-hinh-chu-ga-con-1.jpg",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "397": {
    "inputs": {
      "device": "cuda:0",
      "image": [
        "396",
        0
      ]
    },
    "class_type": "BiRefNet",
    "_meta": {
      "title": "BiRefNet Segmentation"
    }
  },
  "398": {
    "inputs": {
      "width": 576,
      "height": 768,
      "padding": 50,
      "image": [
        "396",
        0
      ],
      "mask": [
        "397",
        0
      ]
    },
    "class_type": "Image Adaptive Crop With Mask",
    "_meta": {
      "title": "Image Adaptive Crop With Mask"
    }
  },
  "399": {
    "inputs": {
      "pixels": [
        "398",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "400": {
    "inputs": {
      "images": [
        "398",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "401": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 20219230480582,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "410",
        5
      ],
      "model": [
        "406",
        0
      ],
      "positive": [
        "389",
        0
      ],
      "negative": [
        "390",
        0
      ],
      "bbox_detector": [
        "402",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "402": {
    "inputs": {
      "model_name": "bbox/face_yolov8m.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "403": {
    "inputs": {
      "filename_prefix": "ComfyUI",
      "images": [
        "412",
        0
      ]
    },
    "class_type": "SaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "404": {
    "inputs": {
      "stop_at_clip_layer": -2,
      "clip": [
        "393",
        1
      ]
    },
    "class_type": "CLIPSetLastLayer",
    "_meta": {
      "title": "CLIP Set Last Layer"
    }
  },
  "405": {
    "inputs": {
      "image": "Screenshot 2024-07-29 224916.png",
      "upload": "image"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "406": {
    "inputs": {
      "weight": 0.4,
      "weight_type": "linear",
      "combine_embeds": "concat",
      "start_at": 0,
      "end_at": 1,
      "embeds_scaling": "V only",
      "model": [
        "407",
        0
      ],
      "ipadapter": [
        "407",
        1
      ],
      "image": [
        "405",
        0
      ],
      "clip_vision": [
        "408",
        0
      ]
    },
    "class_type": "IPAdapterAdvanced",
    "_meta": {
      "title": "IPAdapter Advanced"
    }
  },
  "407": {
    "inputs": {
      "preset": "PLUS FACE (portraits)",
      "model": [
        "393",
        0
      ]
    },
    "class_type": "IPAdapterUnifiedLoader",
    "_meta": {
      "title": "IPAdapter Unified Loader"
    }
  },
  "408": {
    "inputs": {
      "clip_name": "CLIP-ViT-H-14-laion2B-s32B-b79K.safetensors"
    },
    "class_type": "CLIPVisionLoader",
    "_meta": {
      "title": "Load CLIP Vision"
    }
  },
  "409": {
    "inputs": {
      "images": [
        "410",
        5
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "Preview Image"
    }
  },
  "410": {
    "inputs": {
      "seed": 824239455640584,
      "steps": 30,
      "cfg": 7,
      "sampler_name": "dpmpp_sde",
      "scheduler": "karras",
      "denoise": 1,
      "preview_method": "auto",
      "vae_decode": "true",
      "model": [
        "391",
        0
      ],
      "positive": [
        "389",
        0
      ],
      "negative": [
        "390",
        0
      ],
      "latent_image": [
        "388",
        0
      ]
    },
    "class_type": "KSampler (Efficient)",
    "_meta": {
      "title": "KSampler (Efficient)"
    }
  },
  "411": {
    "inputs": {
      "vae_name": "anythingKlF8Anime2VaeFtMse840000_clearvae23.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "412": {
    "inputs": {
      "guide_size": 512,
      "guide_size_for": true,
      "max_size": 1024,
      "seed": 203854136457084,
      "steps": 30,
      "cfg": 5,
      "sampler_name": "dpmpp_2m",
      "scheduler": "karras",
      "denoise": 0.5,
      "feather": 5,
      "noise_mask": true,
      "force_inpaint": true,
      "bbox_threshold": 0.5,
      "bbox_dilation": 10,
      "bbox_crop_factor": 3,
      "sam_detection_hint": "center-1",
      "sam_dilation": 0,
      "sam_threshold": 0.93,
      "sam_bbox_expansion": 0,
      "sam_mask_hint_threshold": 0.7,
      "sam_mask_hint_use_negative": "False",
      "drop_size": 10,
      "wildcard": "",
      "cycle": 1,
      "inpaint_model": false,
      "noise_mask_feather": 20,
      "image": [
        "401",
        0
      ],
      "model": [
        "406",
        0
      ],
      "positive": [
        "389",
        0
      ],
      "negative": [
        "390",
        0
      ],
      "bbox_detector": [
        "413",
        0
      ],
      "sam_model_opt": [
        "414",
        0
      ]
    },
    "class_type": "FaceDetailer",
    "_meta": {
      "title": "FaceDetailer"
    }
  },
  "413": {
    "inputs": {
      "model_name": "bbox/hand_yolov8s.pt"
    },
    "class_type": "UltralyticsDetectorProvider",
    "_meta": {
      "title": "UltralyticsDetectorProvider"
    }
  },
  "414": {
    "inputs": {
      "model_name": "sam_vit_b_01ec64.pth",
      "device_mode": "AUTO"
    },
    "class_type": "SAMLoader",
    "_meta": {
      "title": "SAMLoader (Impact)"
    }
  }
}